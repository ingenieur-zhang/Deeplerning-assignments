{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### This submission is for... (*put up to three people*)\n",
        "- Erika Mustermann (87654321)\n",
        "- Max Mustermann (12345678)\n",
        "- Maxine Musterfrau (87651234)"
      ],
      "metadata": {
        "cell_id": "64116a8d660041ad995eee80e9ccbd43",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 153.171875,
        "id": "WExusxG_2tgY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1 - Simple Neural Network"
      ],
      "metadata": {
        "formattedRanges": [],
        "cell_id": "b471288db3b44cfb992b6c07f7c3fd83",
        "tags": [],
        "is_collapsed": false,
        "deepnote_cell_type": "text-cell-h1",
        "id": "w6spdQYQ2tgd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the first exercise, you will incrementally build a simple neural network from scratch with numpy. Our first challenge is solving the XOR task that you've seen in the lecture, before we move to a slightly more complex problem, namely the Iris dataset."
      ],
      "metadata": {
        "formattedRanges": [],
        "cell_id": "7b2ab899b012471f80f00f1ff3b7992a",
        "tags": [],
        "is_collapsed": false,
        "deepnote_cell_type": "text-cell-p",
        "id": "xWJ_eyKS2tge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can receive up to three points for your implementation of Exercise 1. After that you can either choose Exercise 2A or Exercise 2B to receive another three points. In sum, you can get up to six bonus points for the exam."
      ],
      "metadata": {
        "formattedRanges": [],
        "cell_id": "38a559d784cb43709780881a4c56aedd",
        "tags": [],
        "is_collapsed": false,
        "deepnote_cell_type": "text-cell-p",
        "id": "fDTGjoEB2tge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Exercise 2A**: Building a Transformer network with PyTorch applied in the NLP domain\n",
        "- **Exercise 2B**: Building a GAN network with PyTorch applied in the image domain"
      ],
      "metadata": {
        "cell_id": "6984113451a342818dac8a8e0bd440ac",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 94.78125,
        "id": "VGOlYN9Z2tgf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important Notice**: Throughout the notebook, basic structures are provided such as functions and classes without bodies or partial bodies, and variables that you need to assign to. **Don't change the names of functions, variables, and classes - and make sure that you are using them!** You're allowed to introduce helper variables and functions. Occasionally, we use **type annotations** that you should follow. They are not enforced by Python. Whenenver you see an ellipsis `...` or TODO comment, you're supposed to insert code."
      ],
      "metadata": {
        "cell_id": "dfb46b3d43444b14961ecd9967845ea1",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 141.953125,
        "id": "pOIoDJv_2tgf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XOR Task"
      ],
      "metadata": {
        "formattedRanges": [],
        "cell_id": "10a3e57eb4844285b70b25bcd41c834e",
        "tags": [],
        "is_collapsed": false,
        "deepnote_cell_type": "text-cell-h2",
        "id": "M8QD3-q02tgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XOR (exclusive OR) is a logic function that gives 1 as an output when the number of true inputs is odd, otherwise it outputs a 0. Our goal is to model this function using neurons. We'll start with a single neuron."
      ],
      "metadata": {
        "formattedRanges": [],
        "cell_id": "84d575c151ba447192c5bc8d6bf696e1",
        "tags": [],
        "is_collapsed": false,
        "deepnote_cell_type": "text-cell-p",
        "id": "B4XIvrKj2tgh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://community.anaplan.com/t5/image/serverpage/image-id/29631i3AA6C01377A8550F/image-size/large?v=v2&px=999\" width=\"250\"/></center>"
      ],
      "metadata": {
        "cell_id": "c9900820a8204251a55585f98126c98e",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 276.84375,
        "id": "ekh7hOIk2tgh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A Single Neuron (Perceptron)"
      ],
      "metadata": {
        "formattedRanges": [],
        "is_collapsed": false,
        "cell_id": "64f8dfc7d36a4abfa267750813edfa2d",
        "tags": [],
        "deepnote_cell_type": "text-cell-h2",
        "id": "txWXYqry2tgi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with importing some necessary dependencies that we will need throughout the notebook."
      ],
      "metadata": {
        "formattedRanges": [],
        "cell_id": "eeef0f122a2e45be9ab17634e036b17f",
        "tags": [],
        "is_collapsed": false,
        "deepnote_cell_type": "text-cell-p",
        "id": "RS83rXR82tgi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "95e9b1a1f9494ea89f65aee06be516dc",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 61,
        "id": "-nrjhMBO2tgj"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the first part of this exercise you'll build a perceptron, a single neuron, that takes both binary input values and returns a binary output value.\n",
        "\n",
        "<center><img src=\"https://i.stack.imgur.com/eBSki.jpg\" width=\"280\" />\n",
        "\n",
        "<center><img src=\"\" width=\"280\"/>"
      ],
      "metadata": {
        "cell_id": "39505416089744699256a5844dd99d19",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 252.9375,
        "id": "dlGxHaM72tgk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perceptron can be seen as a single neuron, mapping an input $\\textbf{x}$ to an output $o$ using weights $\\textbf{w}$ and a bias $b$. $\\cdot$ is the dot product."
      ],
      "metadata": {
        "cell_id": "816aceffd3b045029c7d82becc35489e",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 52.390625,
        "id": "I7PmyXz92tgk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$o = \\textbf{w}\\cdot \\textbf{x}+b$"
      ],
      "metadata": {
        "cell_id": "11a6fcabddb143599d0b7bc55f5cc50c",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 52.390625,
        "id": "7UjfCaeq2tgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Perceptron Update Rule\n",
        "\n",
        "In the lecture we learned about the **Perceptron algorithm** / **Perceptron update rule** which we can apply to binary classification problems. Let's use it here to have a first baseline."
      ],
      "metadata": {
        "cell_id": "59d21d8a14cf4ffab7daf30a94c91be8",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 110.78125,
        "id": "QW5c6tT52tgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For classification problems $0>o$ is interpreted as class 1, and $o<0$ is interpreted as class 0. "
      ],
      "metadata": {
        "cell_id": "fe3ac47eb7134fbd9e92b9b887dcda12",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 52.390625,
        "id": "5dNqpPdd2tgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For updating the associated weights, we can use the following update rule:\n",
        "\n",
        "$w_i = w_i + \\nabla w_i$\n",
        "\n",
        "where\n",
        "\n",
        "$\\nabla w_i = \\eta(t-o)x_i$\n",
        "\n",
        "- $t$ is the target\n",
        "- $o$ is the output\n",
        "- $\\eta$ is the learning rate (a small constant)"
      ],
      "metadata": {
        "cell_id": "6454ee77e7e4411f8d341729e17e90a5",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 262.734375,
        "id": "CWqEU0Ds2tgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation of a Perceptron"
      ],
      "metadata": {
        "formattedRanges": [],
        "is_collapsed": false,
        "cell_id": "89280b788ec34866ab8f8ead1cc686c3",
        "tags": [],
        "deepnote_cell_type": "text-cell-h3",
        "id": "oCy6z6Ig2tgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class perceptron_implementation():\n",
        "    def __init__(self):\n",
        "        self.neuron_weights = None\n",
        "        self.bias = None\n",
        "        self.initialize_weights()\n",
        "        \n",
        "    def initialize_weights(self):\n",
        "        # TODO: \n",
        "        # Initialize weights \n",
        "        # For perceptrons, it's possible to initialize the weights with 0\n",
        "        \n",
        "        # END TODO\n",
        "\n",
        "    def forward_pass(self, x):\n",
        "        # TODO\n",
        "        # Implement forward propagation\n",
        "        output = None\n",
        "\n",
        "        # END TODO\n",
        "        return output\n",
        "\n",
        "    def perceptron_update_rule(self, target, prediction, learning_rate = 1):\n",
        "        # TODO\n",
        "        # Perform perceptron update rule that is defined above\n",
        "        # use self.neuron_weights\n",
        "        new_weights = None\n",
        "\n",
        "        # END TODO\n",
        "        self.neuron_weights = new_weights\n",
        "\n",
        "    def train(self, input_data, targets):\n",
        "        \"\"\"\n",
        "        input_data: Multi-dimensional array that contains all inputs\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        # Call the necessary functions to train a single neuron for the given task\n",
        "        # Complete the rest of the code to correctly train the model\n",
        "\n",
        "        # END TODO\n",
        "\n",
        "    def inference(self, input_data):\n",
        "        # TODO\n",
        "        # Test the trained neuron\n",
        "\n",
        "        # END TODO\n",
        "        return output"
      ],
      "metadata": {
        "cell_id": "82eb15fa556f4017824bfc25bdfa9b71",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 871,
        "id": "eNs7zHyT2tgm",
        "outputId": "44626fc5-56dd-4a73-a7c1-58c6c993eedd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-bc71adb8b29a>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    def forward_pass(self, x):\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "formattedRanges": [],
        "cell_id": "75e4622dce1a4d39b1734879909d04d2",
        "tags": [],
        "is_collapsed": false,
        "deepnote_cell_type": "text-cell-h3",
        "id": "5kpPuTNY2tgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "perceptron = perceptron_implementation()\n",
        "\n",
        "# TODO\n",
        "\n",
        "input_data = ...\n",
        "targets = ...\n",
        "\n",
        "# train the corresponding single neuron \n",
        "perceptron.train(input_data, targets)\n",
        "# END TODO"
      ],
      "metadata": {
        "cell_id": "fdac76e29da047579c02f7c61cec4ffc",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 223,
        "id": "k7UXXkGL2tgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "formattedRanges": [],
        "is_collapsed": false,
        "cell_id": "c22027433597480481df2737709639dd",
        "tags": [],
        "deepnote_cell_type": "text-cell-h3",
        "id": "TD9gtzyL2tgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# Test the trained model\n",
        "\n",
        "predictions = perceptron.inference(input_data)\n",
        "print(predictions)\n",
        "# END TODO"
      ],
      "metadata": {
        "cell_id": "c454d52152ce428c81d51ad9f34fd970",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 151,
        "id": "agv_oJ382tgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "formattedRanges": [],
        "cell_id": "85b73e5dbdd14488b836e0dad804b0a3",
        "tags": [],
        "is_collapsed": false,
        "deepnote_cell_type": "text-cell-h3",
        "id": "DcMjb5zH2tgo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For evaluation, we will need to consider appropriate metrics. For classification tasks, **accuracy** is one of the most common metrics.\n",
        "\n",
        "It is defined as:\n",
        "\n",
        "$\\textrm{Accuracy}=\\frac{1}{N}\\sum_i^N1(y_i=\\hat{y}_i)$\n",
        "\n",
        "where $y$ is an array of our target values, and $\\hat{y}$ is an array of our predictions.\n",
        "\n",
        "For accuracy, if outputs are probabilities, there needs to be a threshold for transforming logit predictions to binary `(0,1)` predictions. We will set this threshold to `0.5`. For our perceptron this is not needed, since we already output binary values, however, we will use the `accuracy` function later on, so the predictions should be considered to be probabilities."
      ],
      "metadata": {
        "cell_id": "e76ebc19dbad44988015c11187bd7969",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 243.34375,
        "id": "ETEW4Rhp2tgo"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "65abd9e35d404aa5af91b1ab7d74147b",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 259,
        "id": "xJCPWPYj2tgo"
      },
      "source": [
        "def accuracy(predictions: np.ndarray, targets: np.ndarray, threshold=0.5) -> float:\n",
        "    # TODO\n",
        "    # Implement the accuracy metric\n",
        "    ...\n",
        "    # END TODO\n",
        "    return accuracy_value\n",
        "\n",
        "# TODO\n",
        "# Call accuracy function and provide necessary inputs to calculate accuracy\n",
        "accuracy_value = accuracy(...)\n",
        "print(accuracy_value)\n",
        "# END TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple Neurons"
      ],
      "metadata": {
        "formattedRanges": [],
        "cell_id": "cbdff3e9b9014471a55b2ea4d1221784",
        "tags": [],
        "is_collapsed": false,
        "deepnote_cell_type": "text-cell-h2",
        "id": "YZJLPvp22tgo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The perceptron algorithm can't be generalized to multiple neurons or even layers of neurons, that's why we will now use **backpropagation**. This requires us to have a **loss function**."
      ],
      "metadata": {
        "cell_id": "fd42101ad8624579bd3ba87011aace0d",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 74.78125,
        "id": "d-OvFWl22tgo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our XOR task, it is now our goal is now to build a network akin to this, i.e., a network with three single-neuron hidden layers:\n",
        "\n",
        "<img src=\"https://i.imgur.com/oErVmm2.png\">"
      ],
      "metadata": {
        "cell_id": "a5b08e555e0c4ff6b726f6e609bb8dc9",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 211.0625,
        "id": "I6J7e8Fh2tgo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Backpropagation\n",
        "\n",
        "<center><img src=\"https://i.imgur.com/LgBzpYD.png\" width=\"400\" /></center>"
      ],
      "metadata": {
        "cell_id": "aaab44ed0b0948398d7bc4258dee27ab",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 267.75,
        "id": "jc6g4n0-2tgp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sigmoid Activation Function"
      ],
      "metadata": {
        "formattedRanges": [],
        "is_collapsed": false,
        "cell_id": "207132d07fec4fe887a8ce578307e95a",
        "tags": [],
        "deepnote_cell_type": "text-cell-h3",
        "id": "NHAGXyrR2tgp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a binary classification problem, we can use the sigmoid activation function in the output layer which outputs values in the range of 0 and 1. So, for a positive case (class 1), we can interpret $p_1 = \\sigma(o)$ as the probability of that class, while $p_0 = 1 - p_1$ can be seen the probability of the negative case (class 0)."
      ],
      "metadata": {
        "cell_id": "b75c8af994a84628af79cb6906e06842",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 97.171875,
        "id": "y-LgMkeC2tgp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "f3ee1efc24284dfc836bb18198f6a1e8",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 295,
        "id": "p9nIIYj02tgp"
      },
      "source": [
        "class sigmoid_activation_function():\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        output = None\n",
        "        \n",
        "        # TODO\n",
        "        # implement Sigmoid function for the input_data\n",
        "\n",
        "        # END TODO\n",
        "        return output\n",
        "    \n",
        "    def backward(self, gradients):\n",
        "        ... # calculate the gradients with help of the derivative\n",
        "        return ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Function (Binary Cross Entropy)"
      ],
      "metadata": {
        "formattedRanges": [],
        "is_collapsed": false,
        "cell_id": "6360b3c0360d4193981ac91f621abef8",
        "tags": [],
        "deepnote_cell_type": "text-cell-h3",
        "id": "0AumYOUd2tgp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$L=-\\frac{1}{N}\\sum_{i=0}^Ny_i log(p(y_i))+(1-y_i)log(1-p(y_i))$\n",
        "\n",
        "where $N$ is the batch size."
      ],
      "metadata": {
        "cell_id": "52577f99ec9d4cb5918a774edbe97d63",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 89.390625,
        "id": "Z53G5Bwn2tgp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "f8f969c517f24b28b485149b9bdfd8c5",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 295,
        "id": "BtsbaLuL2tgq"
      },
      "source": [
        "class binary_cross_entropy():\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        loss = None\n",
        "        \n",
        "        # TODO\n",
        "        # implement Binary Cross-Entrops loss function for output, target\n",
        "\n",
        "        # END TODO\n",
        "        return loss\n",
        "    \n",
        "    def backward(self):\n",
        "        ... # calculate the gradients with help of the derivative\n",
        "        return ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing Weights"
      ],
      "metadata": {
        "formattedRanges": [],
        "cell_id": "df9ec443c0284dc4a52c55b8caa88358",
        "tags": [],
        "is_collapsed": false,
        "deepnote_cell_type": "text-cell-h3",
        "id": "UQdPsvFa2tgq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Xavier intitialization is commonly used to initialize the weights of a network. It is a random uniform distribution thatâ€™s bounded between $\\pm\\frac{\\sqrt{6}}{\\sqrt{n_i+n_{i+1}}}$ where $n_i$ is the number of incoming network connections, and $n_{i+1}$ is the number of outgoing network connections."
      ],
      "metadata": {
        "cell_id": "c251b1998be74c5fbc41cd1d47edaa88",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 101.46875,
        "id": "lV-m23SR2tgq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "6b33fda27eb04ef2ad06126f77cc4dfe",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 97,
        "id": "AAONnAXQ2tgq"
      },
      "source": [
        "def xavier_initialization(n_incoming: np.ndarray, n_outgoing: np.ndarray) -> np.ndarray:\n",
        "    \"\"\" Returns a numpy array of initialized weights \"\"\"\n",
        "    ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement Multiple Neurons"
      ],
      "metadata": {
        "formattedRanges": [],
        "cell_id": "2cff5be7ecc843bf9056d30bf7fdb861",
        "tags": [],
        "is_collapsed": false,
        "deepnote_cell_type": "text-cell-h3",
        "id": "SqM-XKIr2tgq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feed-Forward Layer\n",
        "\n",
        "A feed-forward layer applies a linear transformation to the input $x$ using a weight matrix $\\textbf{W}$ and a bias vector $b$:\n",
        "\n",
        "$z = x\\textbf{W}^T+b$"
      ],
      "metadata": {
        "cell_id": "153fa06156c54bdc8e8b10a85a586dd5",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 341.78125,
        "id": "oaSce1El2tgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class multi_neuron_implementation():\n",
        "    def __init__(self, number_of_neurons, loss_function, output_activation_function):\n",
        "        self.neuron_weights = None\n",
        "        self.number_of_neurons = number_of_neurons\n",
        "        self.loss_function = loss_function\n",
        "        self.output_activation_function = output_activation_function\n",
        "\n",
        "    def forward_pass(self, x):\n",
        "        # TODO\n",
        "        # Implement forward propagation\n",
        "        output = None\n",
        "\n",
        "        # END TODO\n",
        "        return output\n",
        "\n",
        "    def backward_pass(self, x):\n",
        "        # TODO\n",
        "        # Perform backpropagation by calculating derivative\n",
        "        output = None\n",
        "\n",
        "        # END TODO\n",
        "        return output\n",
        "\n",
        "    def update_parameter(self, loss, derivative, learning_rate = 1):\n",
        "        # TODO\n",
        "        # Perform weight update\n",
        "        # use self.neuron_weights\n",
        "        new_weights = None\n",
        "\n",
        "        # END TODO\n",
        "        self.neuron_weights = new_weights\n",
        "\n",
        "    def train(self, input_data, targets):\n",
        "        # TODO\n",
        "        # Call the necessary functions to train the model with multiple neurons for the given task\n",
        "        # Complete the rest of the code to correctly train the model\n",
        "\n",
        "        # END TODO\n",
        "\n",
        "    def inference(self, input_data):\n",
        "        # TODO\n",
        "        # Test the trained model\n",
        "\n",
        "        # END TODO\n",
        "        return output"
      ],
      "metadata": {
        "cell_id": "a03ea752340644e699dd3a11f8f5dc74",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 853,
        "id": "zUBUSQVq2tgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "formattedRanges": [],
        "cell_id": "d00fc9e07a9e4453beacaaa916ba6f75",
        "tags": [],
        "is_collapsed": false,
        "deepnote_cell_type": "text-cell-h3",
        "id": "bceJ5_Or2tgr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "multi_neuron = multi_neuron_implementation(...)\n",
        "\n",
        "# TODO\n",
        "# train the corresponding single neuron \n",
        "multi_neuron.train(input_data, targets)\n",
        "# END TODO"
      ],
      "metadata": {
        "cell_id": "0163bbd275bd4f11a64251c9b1bee75d",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 151,
        "id": "oo-iCWs12tgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "formattedRanges": [],
        "is_collapsed": false,
        "cell_id": "1bd4b4a7b8094b52a0c7580307ee0d92",
        "tags": [],
        "deepnote_cell_type": "text-cell-h3",
        "id": "Y8n4VeLH2tgr"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "06c36f9c0ecc42a7a71d82cf0f76a0a4",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 133,
        "id": "fnnCxdic2tgr"
      },
      "source": [
        "# TODO\n",
        "# Test the trained model\n",
        "predictions = multi_neuron.inference(input_data)\n",
        "print(predictions)\n",
        "# END TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "is_collapsed": false,
        "cell_id": "fe84c072be20440fbea2c82fa9fb57b2",
        "tags": [],
        "formattedRanges": [],
        "deepnote_cell_type": "text-cell-h3",
        "id": "8d6t_mIT2tgs"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "bf82e1a3072f47c295a5a557ca9a32e8",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 133,
        "id": "Ot4Dw5Cb2tgs"
      },
      "source": [
        "# TODO\n",
        "# Call accuracy function and provide necessary inputs to calculate accuracy\n",
        "accuracy_value = accuracy(...)\n",
        "print(accuracy_value)\n",
        "# END TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Layer Perceptron (MLP)"
      ],
      "metadata": {
        "formattedRanges": [],
        "cell_id": "ed9c69e1f9d7460b89f6899e938a68bf",
        "tags": [],
        "is_collapsed": false,
        "deepnote_cell_type": "text-cell-h2",
        "id": "9zwDNf042tgs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's generalize even further and build a network with an arbitrary (parametrized) number of hidden layers and hidden dimensions. For the XOR task specifically, we will consider a network with three hidden layers and a hidden dimension of three. We will also add an activiation function to introduce nonlinearity in our hidden layers.\n",
        "\n",
        "<img src=\"https://i.imgur.com/IUQ05Ol.png\">"
      ],
      "metadata": {
        "cell_id": "84f317b46e3b483fae7df63d0dc25b17",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 414.625,
        "id": "c0ijjfgD2tgs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementation"
      ],
      "metadata": {
        "is_collapsed": false,
        "cell_id": "e731239aef1b43b6a1b58b47a03f0682",
        "tags": [],
        "formattedRanges": [],
        "deepnote_cell_type": "text-cell-h3",
        "id": "RJYI8_GE2tgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP_implementation():\n",
        "    def __init__(self,\n",
        "        hidden_layers,\n",
        "        hidden_layers_size,\n",
        "        hidden_activation_func,\n",
        "        output_activation_function,\n",
        "        loss_function,\n",
        "    ):\n",
        "        self.hidden_layers = hidden_layers\n",
        "        self.hidden_layers_size = hidden_layers_size\n",
        "        self.hidden_activation_func = hidden_activation_func\n",
        "        self.loss_function = loss_function\n",
        "        self.output_activation_function = output_activation_function\n",
        "        # TODO\n",
        "        # Implement your MLP model \n",
        "\n",
        "        # END TODO\n",
        "\n",
        "    def forward_pass(self, x):\n",
        "        # TODO\n",
        "        # Implement forward propagation\n",
        "        output = None\n",
        "\n",
        "        # END TODO\n",
        "        return output\n",
        "\n",
        "    def backward_pass(self, x):\n",
        "        # TODO\n",
        "        # Perform backpropagation by calculating derivative\n",
        "        output = None\n",
        "\n",
        "        # END TODO\n",
        "        return output\n",
        "\n",
        "    def update_parameter(self, loss, derivative, learning_rate = 1):\n",
        "        # TODO\n",
        "        # Perform weight update\n",
        "\n",
        "        # END TODO\n",
        "\n",
        "    def train(self, input_data, targets):\n",
        "        # TODO\n",
        "        # Call the necessary functions to train the model for the given task\n",
        "        # Complete the rest of the code to correctly train the model\n",
        "\n",
        "        # END TODO\n",
        "\n",
        "    def inference(self, input_data):\n",
        "        # TODO\n",
        "        # Test the trained model\n",
        "\n",
        "        # END TODO\n",
        "        return output"
      ],
      "metadata": {
        "cell_id": "163c21039acf4345856cc2d1fec878d7",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 997,
        "id": "7kB5OBuF2tgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding Nonlinearity"
      ],
      "metadata": {
        "formattedRanges": [],
        "cell_id": "5f712a71667a48fba5caf101171168cb",
        "tags": [],
        "is_collapsed": false,
        "deepnote_cell_type": "text-cell-h3",
        "id": "KbVJQkvx2tgt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time, you need to implement and apply nonlinearity. For this, you should implement Rectified Linear Unit (ReLU) and apply it to provide nonlinearity to the network.\n",
        "\n",
        "Basically, ReLU activation function is a mathematical operation that processes the input data and checks whether the input is positive or not. If it is positive, then it does not change anything. Otherwise, ReLU outputs zero. \n",
        "\n",
        "When we examine the ReLU behavior, it looks like it is the combination of two different linear functions. This property makes the training easier yet effective since ReLU does not have any learnable parameters as well as easy to apply because of combination of two simple linear functions. The following equation and figure show how ReLU acts.\n",
        "\n",
        "$$ y = max(0, x) $$\n",
        "\n",
        "<center><figure><img src=\"https://machinelearningmastery.com/wp-content/uploads/2018/10/Line-Plot-of-Rectified-Linear-Activation-for-Negative-and-Positive-Inputs.png\" width=\"450\"/><figcaption>Graph of the ReLU activation function. <a href=\"https://machinelearningmastery.com/wp-content/uploads/2018/10/Line-Plot-of-Rectified-Linear-Activation-for-Negative-and-Positive-Inputs.png\">Image is taken from</a></figcaption></figure></center>"
      ],
      "metadata": {
        "cell_id": "a4f3d994ea8c4a7892f2c1f1f1d71082",
        "tags": [],
        "owner_user_id": "06b28ca6-80fe-4ecd-a509-50438de77bba",
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 651.015625,
        "id": "TJx_JZbl2tgt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "9ea233c84766446f966fa64794d2d63f",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 349,
        "id": "8W-5egab2tgt"
      },
      "source": [
        "# You need to use the same implementation that you do in the previous task, if you need.\n",
        "# You only need to include ReLU activation function in your implementation.\n",
        "\n",
        "class relu_activation_function():\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        output = None\n",
        "        \n",
        "        # TODO\n",
        "        # implement ReLU function for the input_data\n",
        "\n",
        "        # END TODO\n",
        "        return output\n",
        "    \n",
        "    def backward(self, gradients):\n",
        "        ... # calculate the gradients with help of the derivative\n",
        "        return ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLP Inititialization"
      ],
      "metadata": {
        "is_collapsed": false,
        "cell_id": "f3eb7f63f1ef4b76907870481e5dde3e",
        "tags": [],
        "formattedRanges": [],
        "deepnote_cell_type": "text-cell-h3",
        "id": "0l6UYSco2tgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "relu = relu_activation_function()\n",
        "xor_mlp = MLP_implementation(...)"
      ],
      "metadata": {
        "cell_id": "c463f6ca71c64121a27152c58878f53f",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 79,
        "id": "oUEAOGUT2tgt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "formattedRanges": [],
        "cell_id": "43d7dc2d8c4448f7ac6aaf1c084f695a",
        "tags": [],
        "is_collapsed": false,
        "deepnote_cell_type": "text-cell-h3",
        "id": "CyX7sj5Q2tgu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "5f343b3459d246c8a0b8cdd5fc07a6c7",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 115,
        "id": "hoi1RVs_2tgu"
      },
      "source": [
        "# Train the same model with ReLU activation function\n",
        "# TODO\n",
        "\n",
        "# END TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "formattedRanges": [],
        "cell_id": "be53793e58d24e8a847e7819c7c8b3ce",
        "tags": [],
        "is_collapsed": false,
        "deepnote_cell_type": "text-cell-h3",
        "id": "JSdR2IVW2tgu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "04807c8322c24ced887c6cb5df3d7f32",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 115,
        "id": "yKDqNb5f2tgu"
      },
      "source": [
        "# Test and evaluate your new model as in the previous task\n",
        "# TODO\n",
        "\n",
        "# END TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Application"
      ],
      "metadata": {
        "formattedRanges": [],
        "cell_id": "47b0a74bd12b4a3b9a395514d0b7683b",
        "tags": [],
        "is_collapsed": false,
        "deepnote_cell_type": "text-cell-h2",
        "id": "P1rqjyq42tgu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Iris Dataset ðŸŒ·"
      ],
      "metadata": {
        "formattedRanges": [],
        "cell_id": "3f7b87070bb5488eb7116348ba9e5052",
        "tags": [],
        "is_collapsed": false,
        "deepnote_cell_type": "text-cell-h3",
        "id": "tTiNaSeM2tgu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Iris is a genus of hundreds of species of flowering plants with showy flowers. The Iris data set consists of 150 samples from three species of Iris which are hard to distinguish (Iris setosa, Iris virginica and Iris versicolor). There are four features from each sample: the length and the width of the sepals and petals, in centimeters. Based on these features, the goal is to predict which species of Iris the sample belongs to."
      ],
      "metadata": {
        "formattedRanges": [],
        "cell_id": "8d25765564d04e78b0a65b42ba08d5b4",
        "tags": [],
        "is_collapsed": false,
        "deepnote_cell_type": "text-cell-p",
        "id": "gsMh6WiW2tgu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://www.oreilly.com/library/view/python-artificial-intelligence/9781789539462/assets/462dc4fa-fd62-4539-8599-ac80a441382c.png\" width=\"450\"/></center>"
      ],
      "metadata": {
        "cell_id": "46d56be3c85c4d699ed79f2b482a31c0",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 408.875,
        "id": "fftFJ3ed2tgv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Loading Dataset"
      ],
      "metadata": {
        "cell_id": "d853a4f5816e49a6ac838230e0655087",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 62,
        "id": "lIB2XdFb2tgv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "263a53c7e9754ea488149500a7842994",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 187,
        "id": "eMgXi4pF2tgv"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split the dataset into training and test dataset\n",
        "\n",
        "X_train, y_train = ...\n",
        "X_test, y_test = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Softmax"
      ],
      "metadata": {
        "formattedRanges": [],
        "is_collapsed": false,
        "cell_id": "c94f655f005b4452be7c097443ac0911",
        "tags": [],
        "deepnote_cell_type": "text-cell-h3",
        "id": "1DXogR8U2tgv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previously, we only considered a **binary classification problem**. Iris, however, is a **multiclass classification problem** that requires us to distinguish between three classes. For this case, we can use a **Softmax activation function** in the output layer to transform our outputs (logits) to a probability distribution over our classes.\n",
        "\n",
        "Softmax is defined as\n",
        "\n",
        "$\\texttt{softmax}(z)_i=\\frac{e^{z_i}}{\\sum_{j=1}^N e^{z_j}}$"
      ],
      "metadata": {
        "cell_id": "53272c91c1fe430e97623f79398d2da4",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 176.296875,
        "id": "b1d8caEY2tgv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "da6150862276491195d5ef4989a74a88",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 295,
        "id": "4D1DGjVZ2tgv"
      },
      "source": [
        "class softmax_activation_function():\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        output = None\n",
        "        \n",
        "        # TODO\n",
        "        # implement Softmax function for the input_data\n",
        "\n",
        "        # END TODO\n",
        "        return output\n",
        "    \n",
        "    def backward(self, gradients):\n",
        "        ... # calculate the gradients with help of the derivative\n",
        "        return ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Function (Cross-Entropy)"
      ],
      "metadata": {
        "formattedRanges": [],
        "is_collapsed": false,
        "cell_id": "b30a2de548474d448eaabfd6394b62b6",
        "tags": [],
        "deepnote_cell_type": "text-cell-h3",
        "id": "0dpiqGij2tgv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Related to the previous notes about sigmoid and softmax, we now also need to move from a binary cross entropy loss to a more general cross entropy loss for a multiclass classification problem.\n",
        "\n",
        "Cross-Entropy loss is defined as:\n",
        "\n",
        "$L=-\\frac{1}{N}\\sum_{n=0}^{N}\\sum_i y_i log(y_i')$"
      ],
      "metadata": {
        "cell_id": "98898d7f88a745489ee0a3b8b51edc3c",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 148.171875,
        "id": "OjYwy0SP2tgw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "b1ff232f1893454092631f79f9a073bf",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 295,
        "id": "nGpfFjG62tgw"
      },
      "source": [
        "class cross_entropy_loss():\n",
        "\n",
        "    def forward(self, output, target):\n",
        "        loss = None\n",
        "        \n",
        "        # TODO\n",
        "        # implement Cross-Entrops loss function for output, target\n",
        "\n",
        "        # END TODO\n",
        "        return loss\n",
        "    \n",
        "    def backward(self):\n",
        "        ... # calculate the gradients with help of the derivative\n",
        "        return ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Architecture"
      ],
      "metadata": {
        "formattedRanges": [],
        "cell_id": "379f8681cb0f4d54bdb2a83938685764",
        "tags": [],
        "is_collapsed": false,
        "deepnote_cell_type": "text-cell-h3",
        "id": "TaCmfbHE2tgw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will again use an MLP for this task. Intitialize a model with **4 hidden layers** and a **hidden layer size of 24**."
      ],
      "metadata": {
        "cell_id": "a9ab559f86de48bd8f2ee8f13a64fbda",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 52.390625,
        "id": "rT-aJ-3f2tgw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "formattedRanges": [],
        "cell_id": "651c8f0d9ea44ddc9ab7ad6aa7183498",
        "tags": [],
        "is_collapsed": false,
        "deepnote_cell_type": "text-cell-h3",
        "id": "lPRWF1BF2tgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris_mlp = MLP_implementation(...)\n",
        "iris_mlp.train(X_train, y_train)"
      ],
      "metadata": {
        "cell_id": "717328abeaf14a6a929c5a51466451b5",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 79,
        "id": "HVVl8hwC2tgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "cell_id": "e383e8479b24456fb456a2b163651d8e",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 62,
        "id": "NGxuBpxx2tgw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show the overall accuracy of our model on the test dataset. Use the existing `accuracy` function that you implemented earlier."
      ],
      "metadata": {
        "cell_id": "12d63c77198848e08774450159eb4f5b",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 52.390625,
        "id": "kRnjFbCD2tgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "# Test the trained model\n",
        "predictions_MLP = iris_mlp.inference(X_test)\n",
        "print(predictions_MLP)\n",
        "\n",
        "# Call accuracy function and provide necessary inputs to calculate accuracy\n",
        "accuracy_value_MLP = accuracy(...)\n",
        "print(accuracy_value_MLP)\n",
        "\n",
        "# END TODO"
      ],
      "metadata": {
        "cell_id": "f3491bbddfce45f3bbcac76872d99a7a",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 223,
        "id": "9ujmW5mT2tgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print the confusion matrix using `sklearn.metrics.confusion_matrix`."
      ],
      "metadata": {
        "cell_id": "157c73719d994eb1a61b10d78034ae37",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 52.390625,
        "id": "qK4VI1br2tgx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "cell_id": "ce02ae5b924440b398b5ca899d94a6f6",
        "tags": [],
        "deepnote_cell_type": "code",
        "deepnote_cell_height": 97,
        "id": "8Tr40s4f2tgx"
      },
      "source": [
        "import confusion_matrix from sklearn.metrics\n",
        "\n",
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now please also look at the confusion matrix, what can you conclude from it? (no code, write text as part of this question)"
      ],
      "metadata": {
        "cell_id": "cd41f0bd4689449b85001f94469b6dfb",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 52.390625,
        "id": "9gtdQkiC2tgx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "..."
      ],
      "metadata": {
        "cell_id": "9bece33cdd404cd3bbf2c11a7ea557d5",
        "tags": [],
        "deepnote_cell_type": "markdown",
        "deepnote_cell_height": 52.390625,
        "id": "9gykcH6x2tgx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=faa4af3b-d086-4f42-8b7d-d29c91b1d0f6' target=\"_blank\">\n",
        "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
        "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
      ],
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown",
        "id": "-E6ZLcXm2tgx"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "deepnote": {
      "is_reactive": false
    },
    "deepnote_notebook_id": "50c68a69-c321-4e24-b142-cdaa47048c60",
    "deepnote_execution_queue": [],
    "colab": {
      "name": "â€œexercise1.ipynbâ€",
      "provenance": [],
      "collapsed_sections": [
        "QW5c6tT52tgl",
        "oCy6z6Ig2tgm",
        "5kpPuTNY2tgn",
        "TD9gtzyL2tgn",
        "DcMjb5zH2tgo",
        "jc6g4n0-2tgp",
        "NHAGXyrR2tgp",
        "0AumYOUd2tgp",
        "UQdPsvFa2tgq",
        "SqM-XKIr2tgq",
        "oaSce1El2tgq",
        "bceJ5_Or2tgr",
        "8d6t_mIT2tgs",
        "RJYI8_GE2tgs",
        "KbVJQkvx2tgt",
        "0l6UYSco2tgt",
        "CyX7sj5Q2tgu",
        "JSdR2IVW2tgu",
        "tTiNaSeM2tgu",
        "lIB2XdFb2tgv",
        "1DXogR8U2tgv",
        "0dpiqGij2tgv",
        "TaCmfbHE2tgw",
        "lPRWF1BF2tgw",
        "NGxuBpxx2tgw"
      ],
      "toc_visible": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  }
}