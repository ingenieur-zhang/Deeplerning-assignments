{"cells":[{"cell_type":"markdown","source":["#### This submission is for... (*put up to three people*)\n","- Erika Mustermann (87654321)\n","- Max Mustermann (12345678)\n","- Maxine Musterfrau (87651234)"],"metadata":{"cell_id":"e6929dd6290a45bbbeba2dbd64adda8b","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":153.171875,"id":"ljbys5hEeG4L"}},{"cell_type":"markdown","source":["# Exercise 2A - Transformers"],"metadata":{"cell_id":"9339153b4e8f400894090f4cdec8212f","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":82,"id":"6q4bTzqmeG4R"}},{"cell_type":"markdown","source":["In this exercise, you'll implement a basic encoder-only Transformer architecture with PyTorch. We will start with building the basic building blocks and then integrate them into a fully-fleged Transformer model. We train the model to solve a POS-Tagging problem (more on that later). In the previous exercise, you implemented your work in numpy. Now, we will switch to PyTorch, which will track the gradients for us and allows us to focus more on the network itself."],"metadata":{"formattedRanges":[],"cell_id":"eef422cc65524297ab7b1220162873b0","tags":[],"is_collapsed":false,"owner_user_id":"175a6e67-5f66-4c81-960a-2a75fbd3d9af","deepnote_cell_type":"text-cell-p","id":"QNzI2nyKeG4S"}},{"cell_type":"markdown","source":["You can receive up to three points for your implementation of Exercise 2A. Together with Exercise 1, you can get up to six bonus points for the exam."],"metadata":{"cell_id":"64bcb0b3c2bb43a6b1620d47ecea1de6","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":74.78125,"id":"G9QJNcvveG4S"}},{"cell_type":"markdown","source":["**Important Notice**: Throughout the notebook, basic structures are provided such as functions and classes without bodies or partial bodies, and variables that you need to assign to. **Don't change the names of functions, variables, and classes - and make sure that you are using them!** You're allowed to introduce helper variables and functions. Occasionally, we use **type annotations** that you should follow. They are not enforced by Python. Whenenver you see an ellipsis `...` you're supposed to insert code."],"metadata":{"cell_id":"c66a0519697243859f4aa30b0a0e06c0","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":119.5625,"id":"BrJIJ4jxeG4T"}},{"cell_type":"code","metadata":{"cell_id":"1ed55a8f3a8d4f8ea7f58929e119f5ed","tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"aef22c3f","execution_start":1657123526340,"execution_millis":40884,"deepnote_cell_type":"code","deepnote_cell_height":696,"id":"U0lgZpW_eG4U","outputId":"a5426418-6383-4475-f8cb-a7a526b0ad21"},"source":["!pip install torchtext torchdata torchmetrics"],"execution_count":null,"outputs":[{"name":"stdout","text":"Collecting torchtext\n  Downloading torchtext-0.13.0-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting torchdata\n  Downloading torchdata-0.4.0-cp37-cp37m-manylinux2014_x86_64.whl (4.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting torchmetrics\n  Downloading torchmetrics-0.9.2-py3-none-any.whl (419 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.7/419.7 KB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting torch==1.12.0\n  Downloading torch-1.12.0-cp37-cp37m-manylinux1_x86_64.whl (776.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm in /shared-libs/python3.7/py/lib/python3.7/site-packages (from torchtext) (4.64.0)\nRequirement already satisfied: requests in /shared-libs/python3.7/py/lib/python3.7/site-packages (from torchtext) (2.27.1)\nRequirement already satisfied: numpy in /shared-libs/python3.7/py/lib/python3.7/site-packages (from torchtext) (1.21.6)\nRequirement already satisfied: typing-extensions in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from torch==1.12.0->torchtext) (4.2.0)\nRequirement already satisfied: urllib3>=1.25 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from torchdata) (1.26.9)\nCollecting portalocker>=2.0.0\n  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\nRequirement already satisfied: packaging in /shared-libs/python3.7/py/lib/python3.7/site-packages (from torchmetrics) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from packaging->torchmetrics) (3.0.9)\nRequirement already satisfied: certifi>=2017.4.17 in /shared-libs/python3.7/py/lib/python3.7/site-packages (from requests->torchtext) (2022.5.18.1)\nRequirement already satisfied: idna<4,>=2.5 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from requests->torchtext) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /shared-libs/python3.7/py-core/lib/python3.7/site-packages (from requests->torchtext) (2.0.12)\nInstalling collected packages: torch, portalocker, torchtext, torchmetrics, torchdata\n  Attempting uninstall: torch\n    Found existing installation: torch 1.11.0\n    Not uninstalling torch at /shared-libs/python3.7/py/lib/python3.7/site-packages, outside environment /root/venv\n    Can't uninstall 'torch'. No files were found to uninstall.\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchvision 0.12.0 requires torch==1.11.0, but you have torch 1.12.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed portalocker-2.4.0 torch-1.12.0 torchdata-0.4.0 torchmetrics-0.9.2 torchtext-0.13.0\n\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","metadata":{"cell_id":"20b2dbfb9550468d85d409a5a67f64c0","tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"2c538688","execution_start":1657123567236,"execution_millis":810,"owner_user_id":"06b28ca6-80fe-4ecd-a509-50438de77bba","deepnote_cell_type":"code","deepnote_cell_height":149.375,"id":"WbXGvT8QeG4V","outputId":"fb9c65cd-9383-4fed-94ac-259adeb8e373"},"source":["import torch\n","from torch import nn, Tensor"],"execution_count":null,"outputs":[{"name":"stderr","text":"/shared-libs/python3.7/py/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"}]},{"cell_type":"markdown","source":["Let's actually start with a few basic functions that we will need throughout the exercise, namely **Softmax** and **ReLu**.\n","\n","$\\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$\n","\n","$\\text{ReLU}(x) = \\max(0, x)$"],"metadata":{"cell_id":"925794d055334d71a1874bd0928f2358","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":131.1875,"id":"_L7FM1KVeG4W"}},{"cell_type":"code","metadata":{"cell_id":"89cb4d8c8ca04395a96eb2deedea1989","tags":[],"deepnote_cell_type":"code","deepnote_cell_height":133,"id":"2HeonWi-eG4X"},"source":["def softmax(input: Tensor) -> Tensor:\n","    ...\n","\n","def relu(input: Tensor) -> Tensor:\n","    ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Transformer Block"],"metadata":{"cell_id":"65d60a38ea0d41ca934e8a0f782b1672","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":70,"id":"riUJrILneG4X"}},{"cell_type":"markdown","source":["A typical transformer block consists of the following \n","- Multi-Head Attention\n","- Layer Normalization\n","- Linear Layer\n","- Residual Connections"],"metadata":{"cell_id":"090be921347b4f65a3dd0fca98b29c54","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":178.953125,"id":"F7PUG-HpeG4Y"}},{"cell_type":"markdown","source":["<center><img src=\"https://i.imgur.com/ZKgcoe4.png\" alt=\"transformer block visualization\" width=\"200\">"],"metadata":{"cell_id":"f87bf87fea1a44bcbc2d5c044843e007","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":353,"id":"y1f56Bb3eG4Y"}},{"cell_type":"markdown","source":["In the next few subsections, we will build these basic building blocks."],"metadata":{"formattedRanges":[],"cell_id":"1c8bd5f72db0415bbca4cdd48b0ecd52","tags":[],"is_collapsed":false,"deepnote_cell_type":"text-cell-p","id":"hq6cVatJeG4Z"}},{"cell_type":"markdown","source":["### Multi-Head Attention"],"metadata":{"cell_id":"59b1b38243a84b5c9571ddb814cd1a49","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":62,"id":"pzabxkG5eG4Z"}},{"cell_type":"markdown","source":["Multi-Head Attention concatenates the outputs of several so called **attention heads**.\n","\n","$\\textrm{MHA}(Q,K,V) = \\textrm{Concat}(H_1,...,H_h)$"],"metadata":{"cell_id":"a2a24844affe40a89f26ef6ca0346276","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":88.78125,"id":"jCjxfatXeG4Z"}},{"cell_type":"markdown","source":["<center><img src=\"https://www.tensorflow.org/images/tutorials/transformer/multi_head_attention.png\" width=300>"],"metadata":{"cell_id":"ef2e9b876ee44b69a993f15856b206e9","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":376.15625,"id":"mu9pkARYeG4Z"}},{"cell_type":"markdown","source":["One attention head consists of linear projections for each of $Q, K$ and $V$ and an attention mechanism called **Scaled Dot-Product Attention**. The attention mechanism scales down the dot products by $\\sqrt{d_k}$.\n","\n","$\\textrm{Attention}(Q,K,V)=\\textrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$\n","\n","\n","\n","If we assume that $q$ and $v$ are $d_k$-dimensional vectors and its components are independent random variables with mean $0$ and a variance of $d_k$, then their dot product has a mean of $0$ and variance of $d_k$. It is preferred to have a variance of $1$ and that's why we scale them down by $\\sqrt{d_k}$.\n","\n","The dot product $q \\cdot v$ resembles a measure of similarity.\n"],"metadata":{"cell_id":"a045d2caec3e4020a7bb3fee5680442b","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":233.90625,"id":"ExCaY1FfeG4a"}},{"cell_type":"markdown","source":["<center><img src=\"https://www.tensorflow.org/images/tutorials/transformer/scaled_attention.png\" width=\"350\">"],"metadata":{"cell_id":"9495e574d00e4d3798368384c6b3825b","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":421.609375,"id":"uTjlRU18eG4a"}},{"cell_type":"markdown","source":["Let's start implementing these components. Note that our classes inherit from PyTorch's `nn.Module`. These modules allow us to hold our parameters and easily move them to the GPU (with `.to(...)`). It also let's us define the computation that is performed at every call, in the `forward()` method. For example, when we have an `Attention` module, initialize it like `attention = Attention(...)`, we are able to call it with `attention(Q, K, V)` (it'll execute the `forward` function in an optimized way)."],"metadata":{"cell_id":"e3ed8097791f458a85f3aabfd0756d28","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":119.5625,"id":"XfP6LHKQeG4a"}},{"cell_type":"code","metadata":{"cell_id":"2fc20cdd2c4c4e95aa1cb044b1258d63","tags":[],"deepnote_cell_type":"code","deepnote_cell_height":187,"id":"u7vJBzjSeG4a"},"source":["class Attention(nn.Module):\n","    def __init__(self, hidden_n:int):\n","        super().__init__()\n","        self.hidden_n = hidden_n\n","        ...\n","\n","    def forward(self, Q, K, V, mask=None):\n","        ..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"4ad1acddce234ba98f163b58e34c04d1","tags":[],"deepnote_cell_type":"code","deepnote_cell_height":241,"id":"ixkZ0XBieG4b"},"source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, hidden_n:int, h:int = 2):\n","        \"\"\"\n","        hidden_n: hidden dimension\n","        h: number of heads\n","        \"\"\"\n","        super().__init__()\n","        ...\n","    \n","    def forward(self, Q, K, V, mask=None):\n","        ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Layer Normalization"],"metadata":{"cell_id":"96fa5effdd954c498f1efa8cd0580bc3","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":62,"id":"WSlr69VYeG4b"}},{"cell_type":"markdown","source":["From the lecture, remember layer normalization where the values are normalized across the feature dimension, independently for each sample in the batch. For that, first calculate mean and standard-deviation across the feature dimension and then scale them appropriately such that the mean is 0 and the standard deviation is 1. Introduce **two sets of learnable parameters**, one for shifting the mean (addition) and one for scaling the variance (multiplication) the normalized features (i.e., two parameters for each feature). Tip: Use `nn.Parameter` for that."],"metadata":{"cell_id":"fba0740f863c4787aa23494388944b49","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":141.953125,"id":"tgU10Y_CeG4b"}},{"cell_type":"markdown","source":["$y_{\\textrm{norm}}=\\frac{x-\\mu}{\\sqrt{\\sigma+\\epsilon}}$\n","\n","$y=y_{\\textrm{norm}}\\cdot\\beta+\\alpha$"],"metadata":{"cell_id":"5b0e5f8055a945929dd74b990ecd2a89","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":91.5,"id":"4PmZQqcVeG4b"}},{"cell_type":"markdown","source":["<center>\n","<img src=\"https://i.stack.imgur.com/E3104.png\" alt=\"visualization of layer norm vs. batch norm\" width=\"420\">"],"metadata":{"cell_id":"f96e811cce6d457f85eead1edae4692f","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":92.390625,"id":"OS_CYeLWeG4c"}},{"cell_type":"code","metadata":{"cell_id":"6d5af9330ebd43c5be334c4f3922031c","tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"9c4ce5c0","execution_start":1656077679263,"execution_millis":142,"output_cleared":true,"deepnote_cell_type":"code","deepnote_cell_height":346,"id":"uHrDIN-YeG4c"},"source":["class LayerNorm(nn.Module):\n","    def __init__(self, norm_shape):\n","        \"\"\"\n","        norm_shape: The dimension of the layer to be normalized.\n","        \"\"\"\n","        super().__init__()\n","        self.norm_shape = norm_shape\n","        self.alpha = ...\n","        self.beta = ...\n","        self.epsilon = 1e-10\n","\n","    def forward(self):\n","        mean = ...\n","        var = ...\n","        sdt = ...\n","        ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Transformer Block"],"metadata":{"cell_id":"246bb7a8b64f4596b26778aa9ce5bc85","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":62,"id":"nQ6h1eCEeG4c"}},{"cell_type":"markdown","source":["Here, we bring all ingredients together into a single module. Don't forget to add the residual connections."],"metadata":{"cell_id":"93c32b55767c46049ffb13bc012bb500","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":52.390625,"id":"EDN-moFgeG4c"}},{"cell_type":"code","metadata":{"cell_id":"4e040c8ebd5d42ddb8b00090dffdd960","tags":[],"deepnote_cell_type":"code","deepnote_cell_height":241,"id":"RsMe9cKeeG4c"},"source":["class TransformerBlock(nn.Module):\n","    def __init__(self, hidden_n:int, h:int = 2):\n","        \"\"\"\n","        hidden_n: hidden dimension\n","        h: number of heads\n","        \"\"\"\n","        super().__init__()\n","        ...\n","\n","    def forward(self):\n","        ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## A Simple Transformer Architecture"],"metadata":{"cell_id":"add8b0cdf6ef4a79bf2a71788029d617","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":70,"id":"CQAHyKdaeG4d"}},{"cell_type":"markdown","source":["Let's stack our transformer blocks and add an embedding layer for a simple transformer architecture. You are allowed to use `nn.Embedding` here."],"metadata":{"cell_id":"0215525d6258493485ff71f3496128a2","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":74.78125,"id":"JzW1ede_eG4d"}},{"cell_type":"code","metadata":{"cell_id":"613b3d3f1e7145ba8ea535c0a34a66a5","tags":[],"deepnote_cell_type":"code","deepnote_cell_height":277,"id":"5pP_MRi2eG4d"},"source":["class Transformer(nn.Module):\n","    def __init__(self, emb_n: int, hidden_n: int, n:int =3, h:int =2):\n","        \"\"\"\n","        emb_n: number of token embeddings\n","        hidden_n: hidden dimension\n","        n: number of layers\n","        h: number of heads per layer\n","        \"\"\"\n","        super().__init__()\n","        ...\n","\n","    def forward(self):\n","        ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## POS-Tagging"],"metadata":{"formattedRanges":[],"cell_id":"24ce63a017054111a0e7606078f244eb","tags":[],"is_collapsed":false,"deepnote_cell_type":"text-cell-h2","id":"HzZfOsm9eG4d"}},{"cell_type":"markdown","source":["Part-Of-Speech-Tagging (**POS-Tagging**) is a **sequence labeling problem** where we categorize words in a text in correspondence with a particular part of speech (e.g., \"noun\" or \"adjective\"). A few examples and classes are shown in the following table:"],"metadata":{"cell_id":"46e6832e31434597932051c2d7d833db","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":74.78125,"id":"ok5G2JIaeG4d"}},{"cell_type":"markdown","source":["|  POS Tag  |  Description  |  Examples  |\n","|-----------|------------|------------|\n","|  NN | Noun (singular, common) | mass, wind, ...  |\n","|  NNP | Noun (singular, proper) | Obama, Liverpool, ...  |\n","| CD  | Numeral (cardinal)  | 1890, 0.5, ...  |\n","|  DT | Determiner  | all, any, ... |\n","| JJ | Adjective (ordinal) | oiled, third, ... |\n","... many more"],"metadata":{"cell_id":"0b933b07b931423dae80af31abe46b69","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":200.734375,"id":"-V8o1GbSeG4e"}},{"cell_type":"markdown","source":["### CoNLL2000 Dataset"],"metadata":{"formattedRanges":[],"cell_id":"c1ea426748d34a2891270fb4c9189387","tags":[],"is_collapsed":false,"deepnote_cell_type":"text-cell-h3","id":"XVUQe9dkeG4e"}},{"cell_type":"markdown","source":["Let's load our dataset which is the **CoNLL2000 dataset** and look at an example."],"metadata":{"cell_id":"ec19498428f042a09867d13bfcbbeaf1","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":52.390625,"id":"_r4rwBKBeG4e"}},{"cell_type":"code","metadata":{"cell_id":"56f8d97328eb45808c642c232c18ddeb","tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"c7ccc53b","execution_start":1657123575378,"execution_millis":3449,"deepnote_cell_type":"code","deepnote_cell_height":329.375,"id":"WQ5x_OemeG4e","outputId":"b33bfac9-732d-44b5-d37a-8bb452c3722a"},"source":["from torch.utils.data import Dataset, DataLoader\n","from torchtext.datasets import CoNLL2000Chunking\n","import pandas as pd\n","\n","train_df = pd.DataFrame(CoNLL2000Chunking()[0], columns=['words', 'pos_tags', 'chunk'])\n","test_df = pd.DataFrame(CoNLL2000Chunking()[1], columns=['words', 'pos_tags', 'chunk'])\n","\n","train_src, train_tgt = train_df['words'].tolist(), train_df['pos_tags'].tolist()\n","test_src, test_tgt = test_df['words'].tolist(), test_df['pos_tags'].tolist()\n","\n","print(train_src[0])\n","print(train_tgt[0])"],"execution_count":null,"outputs":[{"name":"stdout","text":"['Confidence', 'in', 'the', 'pound', 'is', 'widely', 'expected', 'to', 'take', 'another', 'sharp', 'dive', 'if', 'trade', 'figures', 'for', 'September', ',', 'due', 'for', 'release', 'tomorrow', ',', 'fail', 'to', 'show', 'a', 'substantial', 'improvement', 'from', 'July', 'and', 'August', \"'s\", 'near-record', 'deficits', '.']\n['NN', 'IN', 'DT', 'NN', 'VBZ', 'RB', 'VBN', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NN', 'NNS', 'IN', 'NNP', ',', 'JJ', 'IN', 'NN', 'NN', ',', 'VB', 'TO', 'VB', 'DT', 'JJ', 'NN', 'IN', 'NNP', 'CC', 'NNP', 'POS', 'JJ', 'NNS', '.']\n","output_type":"stream"}]},{"cell_type":"markdown","source":["First, we need to create a vocabulary. Our dataset is already tokenized. However, we need to assign ids to them in order to input them to the embedding layer. We also need the number of embeddings (`num_embeddings`) for the size of our lookup table of `nn.Embedding`.\n","\n","Thus, we will iterate over all sentences replace them with ids and the mapping to our vocabulary. It'll be handy to have two different mappings, from id to token, as well as, from token to id. Note that we will add a special token `<unk>` with id `0` for words that are unknown (that are not in the training dataset but could possibly be in the test dataset)."],"metadata":{"cell_id":"555caa09a6714a63a9cdee39264291f7","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":178.34375,"id":"SgPF-f9zeG4f"}},{"cell_type":"code","metadata":{"cell_id":"9b665796e64d4aa694d1718b5156294d","tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"2276b8b9","execution_start":1656084179768,"execution_millis":1512,"output_cleared":true,"deepnote_cell_type":"code","deepnote_cell_height":148,"id":"bCG2qiIteG4f"},"source":["vocabulary_id2token : dict = {0: '<unk>'}\n","vocabulary_token2id : dict = {'<unk>': 0}\n","\n","for sentence in train_src:\n","    ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's do the same for our classes:"],"metadata":{"cell_id":"eed144592b0a4aa9a752fab5230b27e6","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":52.390625,"id":"W2RZc6REeG4f"}},{"cell_type":"code","metadata":{"cell_id":"a4401fdd42024ef9b0af6c95af4ff963","tags":[],"deepnote_cell_type":"code","deepnote_cell_height":133,"id":"64yisXOJeG4f"},"source":["classes_id2name : dict = {}\n","classes_name2id : dict = {}\n","\n","for sentence in *[train_src, test_src]:\n","    ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, let's use PyTorch's `Dataset` and `DataLoader` for help us batching our data. Let's also replace tokens and classes with our ids. For that, complete `get_token_ids` and `get_class_ids`."],"metadata":{"cell_id":"ef26b718c8654c7cb211c7542430ee79","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":74.78125,"id":"Q1Qg5sF5eG4g"}},{"cell_type":"code","metadata":{"cell_id":"33964355330b45ad98f64b88799e8672","tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"a6bcb220","execution_start":1656333952181,"execution_millis":2,"deepnote_cell_type":"code","deepnote_cell_height":508,"id":"uHPV_8ikeG4g"},"source":["def get_token_ids(src: list[str]) -> list[int]:\n","    ...\n","\n","def get_class_ids(tgt: list[str]) -> list[int]:\n","    ...\n","\n","class ConllDataset(Dataset):\n","  def __init__(self, src, tgt):\n","        self.src = src\n","        self.tgt = tgt\n","\n","  def __len__(self):\n","        return len(self.src)\n","\n","  def __getitem__(self, index):\n","        src = self.src[index]\n","        tgt = self.tgt[index]\n","        \n","        return {\n","            'src': get_token_ids(src),\n","            'tgt': get_class_ids(tgt),\n","        }\n","\n","train_dataset = ConllDataset(train_src, train_tgt)\n","test_dataset = ConllDataset(test_src, test_tgt)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We will use a **batch size of 32**."],"metadata":{"cell_id":"ce970c07a9ca4b6ab5a4ee941f3492b9","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":52.390625,"id":"DJ80xfgweG4g"}},{"cell_type":"code","metadata":{"cell_id":"e66f66e38e4343a8ac9237e4e9bf74e5","tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"f1c68216","execution_start":1656086903728,"execution_millis":49,"deepnote_cell_type":"code","deepnote_cell_height":76,"id":"UIxtPlKLeG4g"},"source":["BATCH_SIZE = 32"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["However, since our examples are of different length, we need to pad shorter examples to the length of the example with the maximum length in our batch. So, let's define a special **padding token** in our vocabulary:"],"metadata":{"cell_id":"a4c1832e408347d89d304e7870522878","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":74.78125,"id":"EfVZiWv0eG4g"}},{"cell_type":"code","metadata":{"cell_id":"00825cb3c6194a91b8cc03eee1a61e7c","tags":[],"deepnote_cell_type":"code","deepnote_cell_height":115,"id":"6TOptirHeG4g"},"source":["padding_token = ...\n","\n","vocabulary_id2token ...\n","vocabulary_token2id ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `collate_fn` is the function that actually receives a batch and needs to add the padding tokens, then returns `src` and `tgt` as `Tensor`s of size `[B, S]` where `B` is our batch size and `S` our maximum sequence length. This function should additionally return a `mask`, a `Tensor` with binary values to indicate whether the specific element is a padding token or not (0 if it's a padding token, 1 if not), such that we can ignore padding tokens in our attention mechanism and loss calculation. "],"metadata":{"cell_id":"c7b65a2d6b654c8bbe59ad7432758a8a","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":119.5625,"id":"6e5qKqOVeG4h"}},{"cell_type":"code","metadata":{"cell_id":"9266354695a646ebbd776e4290baa002","tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"d3107fb6","execution_start":1656086903777,"execution_millis":0,"deepnote_cell_type":"code","deepnote_cell_height":238,"id":"gQfI3f56eG4h"},"source":["def collate_fn(batch: list[dict]) -> dict[str, Tensor]:\n","    \"\"\"\n","    batch: list of dictionaries with keys src and tgt (as defined in ConllDataset)\n","    \"\"\"\n","    ...\n","    return {\n","        'src': src,\n","        'tgt': tgt,\n","        'mask': mask,\n","    }"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["With that, we can use PyTorch's `DataLoader` which will shuffle and batch our data automatically."],"metadata":{"cell_id":"27c262f705004bac83e8306a99ba8bcb","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":52.390625,"id":"qFkic8QheG4h"}},{"cell_type":"code","metadata":{"cell_id":"110b64825faf4982943ed9eff63ed0ff","tags":[],"deepnote_to_be_reexecuted":false,"source_hash":"50d4e2b2","execution_start":1656086903778,"execution_millis":0,"deepnote_cell_type":"code","deepnote_cell_height":94,"id":"RY27kDyheG4h"},"source":["train_data_loader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=True)\n","test_data_loader = DataLoader(test_dataset, collate_fn=collate_fn, batch_size=BATCH_SIZE, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Architecture"],"metadata":{"formattedRanges":[],"cell_id":"803ef4d85f0a4e2fbcb2ee182520c7f5","tags":[],"is_collapsed":false,"deepnote_cell_type":"text-cell-h3","id":"-E9yfC0WeG4h"}},{"cell_type":"markdown","source":["Let's build a transformer model with three layers, three attention heads and an embedding dimension of 128. Also, let's not forget to add a classification head to our model."],"metadata":{"cell_id":"06f55de40ea248099cbf94bacf9b1268","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":74.78125,"id":"gZYABczXeG4i"}},{"cell_type":"code","metadata":{"cell_id":"5f1f177f8cf343f1bd763f05a8c8703d","tags":[],"deepnote_cell_type":"code","deepnote_cell_height":223,"id":"H4662bygeG4i"},"source":["class CoNLL2000Transformer:\n","    def __init__(self, transformer, ...):\n","        super().__init__()\n","        self.transformer = transformer\n","        self.classification_layer = ...\n","\n","    def forward(self):\n","        ...\n","\n","model = CoNLL2000Transformer(Transformer(...), ...)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training"],"metadata":{"cell_id":"f5aafb3762974b07be3a8a89899db999","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":62,"id":"_t5d-RPIeG4j"}},{"cell_type":"markdown","source":["Initialize the **AdamW** optimizer from the `torch.optim` module and choose the most appropriate loss function for our task."],"metadata":{"cell_id":"965bfa0ea57d4052aabfdba6904f65de","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":52.390625,"id":"ipR3YN5reG4j"}},{"cell_type":"code","metadata":{"cell_id":"5101fa497d074916b870e1e4fe9147d4","tags":[],"deepnote_cell_type":"code","deepnote_cell_height":79,"id":"a-_uDeLceG4j"},"source":["optimizer = ....\n","criterion = ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Build a basic training loop and train the network for three epochs.\n","- Use everything we've built to far, including `train_data_loader`, `model`, `optimizer` and `criterion`.\n","- At every 50th step print the average loss of the last 50 steps. \n","- It is suggested to make a basic training procedure to work on the CPU first. Once it successfully runs on the CPU, you can switch to the GPU (click on change runtime and add an hardware accelerator if you use Colab) and run for the whole three epochs. Note: For this to work, you need to transfer the `model` and the input tensors to the GPU memory. This simply works by calling `.to(device)` on the model and tensors, where `device` and either be `cpu` or `cuda` (for the GPU)."],"metadata":{"cell_id":"de1bb8eda67644db93779c72899e588c","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":220.734375,"id":"RCrj-lUmeG4k"}},{"cell_type":"code","metadata":{"cell_id":"19af38a10aa64403b865dde3603d43e3","tags":[],"deepnote_cell_type":"code","deepnote_cell_height":169,"id":"UdDsp6G_eG4k"},"source":["DEVICE = 'cpu' # later replace with 'cuda' for GPU\n","EPOCHS = 3\n","\n","model = model.to(device)\n","\n","for epoch in range(EPOCHS):\n","    ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluation"],"metadata":{"cell_id":"2f12827ec5ad4a4ba18ae8d260857cd8","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":62,"id":"WbPEbZDaeG4k"}},{"cell_type":"markdown","source":["Let's see what's the accuracy is of our model. Since we already implemented accuracy in the previous exercise, we'll now let you use the torchmetrics package."],"metadata":{"cell_id":"23af9a15c9e84bd3bea4a62c7c7e9459","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":74.78125,"id":"FCyTjeMfeG4k"}},{"cell_type":"code","metadata":{"cell_id":"18891986243d49afa9305716cbd96aa7","tags":[],"deepnote_cell_type":"code","deepnote_cell_height":97,"id":"SuN7Gug4eG4k"},"source":["from torchmetrics import Accuracy\n","\n","accuracy = Accuracy(average='micro')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Calculate the average accuracy of all examples in the test dataset."],"metadata":{"cell_id":"1cd5f69a8dcd41f8918bc9946a1b57e3","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":52.390625,"id":"8SM_npzmeG4l"}},{"cell_type":"code","metadata":{"cell_id":"f73337d4e25a494e9c1b138823bc1d22","tags":[],"deepnote_cell_type":"code","deepnote_cell_height":61,"id":"WKBdPeaeeG4l"},"source":["..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's also look at the accuracy **for each class separately**:"],"metadata":{"cell_id":"d8a4e62ebfb64dc5a6659df362eaeb50","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":52.390625,"id":"YXGAAfE-eG4l"}},{"cell_type":"code","metadata":{"cell_id":"98b6c3390d1b42d2bd9b2e91608625aa","tags":[],"deepnote_cell_type":"code","deepnote_cell_height":61,"id":"is0ozqEjeG4m"},"source":["..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Positional Embeddings"],"metadata":{"formattedRanges":[],"cell_id":"c92bfb82034344bb9cd220de8e4e157c","tags":[],"is_collapsed":false,"deepnote_cell_type":"text-cell-h2","id":"I-nAu5FdeG4m"}},{"cell_type":"markdown","source":["The attention mechanism does not consider the position of the tokens which hurts its performance for many problems. We can solve this issue in several ways. We can either add a positional encoding (via trigonometric functions) or we can learn positional embeddings along the way, in a similar way as BERT does. Here, we will add learnable positional embeddings to our exisisting model with another embedding layer."],"metadata":{"cell_id":"070435ca57a94f1ea6e71050d57619aa","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":119.5625,"id":"zJh6RqOPeG4m"}},{"cell_type":"markdown","source":["The longest sequence in our dataset has 78 tokens (you can trust us on that). So, let's set the number of embeddings for our positional embedding layer to that number. Again, you should use `nn.Embedding`."],"metadata":{"cell_id":"5ff82560d6f24630a9bed80d6b38b7f1","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":74.78125,"id":"xS40xdgeeG4n"}},{"cell_type":"markdown","source":["Copy the inner parts of your `Transformer` class and add positional embeddings to it."],"metadata":{"cell_id":"0363608d5a9a48f5be6f2e6ddbd7397c","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":52.390625,"id":"HjGpmUAweG4n"}},{"cell_type":"code","metadata":{"cell_id":"129e0b2b46c8494c90491fe2d2e137f0","tags":[],"deepnote_cell_type":"code","deepnote_cell_height":313,"id":"tR6qzuq2eG4n"},"source":["class TransformerPos(nn.Module):\n","    def __init__(self, emb_n: int, pos_emb_n: int, hidden_n: int, n:int =3, h:int =2):\n","        \"\"\"\n","        emb_n: number of token embeddings\n","        pos_emb_n: number of position embeddings\n","        hidden_n: hidden dimension\n","        n: number of layers\n","        h: number of heads per layer\n","        \"\"\"\n","        super().__init__()\n","        self.positional_embeddings = ...\n","        ...\n","\n","    def forward(self):\n","        ..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"b398b56387f54999966ee9d22a0150b1","tags":[],"deepnote_cell_type":"code","deepnote_cell_height":61,"id":"RhlhVgYseG4n"},"source":["model_pos = CoNLL2000Transformer(TransformerPos(...), ...)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training"],"metadata":{"cell_id":"d8f6b79fce144176832210f7a1494288","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":62,"id":"ZsejIHwjeG4n"}},{"cell_type":"markdown","source":["Same procedure as before. Let's reinitialize our optimizer and our loss function and run the same training loop with our new model `model_pos`."],"metadata":{"cell_id":"507700c451e842c4b9dbdd02857b234d","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":74.78125,"id":"9oOhcmX1eG4o"}},{"cell_type":"code","metadata":{"cell_id":"7b2c6541371f4e99bb0acfe1212c732d","tags":[],"deepnote_cell_type":"code","deepnote_cell_height":79,"id":"jVjkzqJTeG4o"},"source":["optimizer = ....\n","criterion = ..."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"fac3717095914132b36cbfd70c702715","tags":[],"deepnote_cell_type":"code","deepnote_cell_height":61,"id":"_m2Dr_HceG4o"},"source":["..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluation"],"metadata":{"cell_id":"2dfedd24910c45689ed2a3d4ed804af6","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":62,"id":"kyyOBPI9eG4o"}},{"cell_type":"markdown","source":["Now, let's check if our performance on the accuracy got improved."],"metadata":{"cell_id":"472e05fab0d149b8b269a12d8e363b85","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":52.390625,"id":"O2p4L7gMeG4p"}},{"cell_type":"code","metadata":{"cell_id":"38e0fe847e464f59b1861a749fc43811","tags":[],"deepnote_cell_type":"code","deepnote_cell_height":61,"id":"887sGAJMeG4q"},"source":["..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Again, let's also check each class. Which classes got improved the most by adding positional embeddings?"],"metadata":{"cell_id":"a49fe34010574a0096a020ead994158d","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":52.390625,"id":"GhWRR7GzeG4q"}},{"cell_type":"code","metadata":{"cell_id":"d107d15cf5fe4ee396f6733e093873cc","tags":[],"deepnote_cell_type":"code","deepnote_cell_height":61,"id":"Drl1hQTVeG4q"},"source":["..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The last question in this assignment doesn't require you to code anything. Instead, you're asked to point out possible issues with our current approach and name potential improvements. \n","* ...\n","* ...\n","* ..."],"metadata":{"cell_id":"bfece568a8c1460fafd968adf1972f19","tags":[],"deepnote_cell_type":"markdown","deepnote_cell_height":175.953125,"id":"Pgd2imPueG4r"}},{"cell_type":"markdown","source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=faa4af3b-d086-4f42-8b7d-d29c91b1d0f6' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"],"metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown","id":"xIrSHuRKeG4r"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"orig_nbformat":2,"deepnote":{},"deepnote_notebook_id":"df3e6be7-b747-492e-86ed-19bc62a6bb4c","deepnote_execution_queue":[],"colab":{"name":"“exercise2a.ipynb”的副本","provenance":[{"file_id":"1EA0Pj4JX8xhzRwmzO_YnaUQLFYGIr8GY","timestamp":1659475888796}]}}}